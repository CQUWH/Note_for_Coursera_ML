{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-6 梯度下降的直观理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降算法迭代公式如下：\n",
    "\n",
    "$$\n",
    "\\theta_j:=\\theta_j-\\alpha\\frac{\\partial}{\\partial \\theta_j}J(\\theta_j)\n",
    "$$\n",
    "\n",
    "其中 $\\alpha$ 为学习率，即一次迭代过程中往**负梯度方向**前进的步长。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习率 $\\alpha$ 对迭代过程的影响\n",
    "\n",
    "如果学习率 $\\alpha$ 太小将导致收敛速度太慢，需要迭代许多次才能收敛到一个极小点。  \n",
    "如果学习率 $\\alpha$ 太大可能会越过最低点导致不收敛，甚至发散。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为什么不改变 $\\alpha$ 的值就能最终收敛到极小值点\n",
    "\n",
    "当 $\\theta_j$ 越靠近极小值点，梯度就会越接近于0，这时就算 $\\alpha$ 保持不变，$\\alpha\\frac{\\partial}{\\partial \\theta_j}J(\\theta_j)$ 也会趋近于0，此时 $\\theta_j$ 就可以维持在极小值点附近。所以整体来看，虽然学习率在迭代过程中是一定的，但迭代的步长（这里的步长指学习率乘以梯度）却在大体上是逐渐缩短的。所以梯度下降法没有必要再迭代过程中减小 $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降法的重要性\n",
    "\n",
    "梯度下降法可以用来最小化任何代价函数，而不仅仅是线性回归中的代价函数。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
